{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will *naively* implement a neural network to approximate the Q function. Since value function approximation can handle large state spaces (even continuous), we will use a continuous state space. We will use the *CartPole* environment from Gymnasium (subsequent version of OpenAI Gym).\n",
    "\n",
    "The core idea of Q learning with Q function approximation is to learn a Q function $\\hat{q}(s,a,w)$ (*with optimal policy*) that approximates the true Q function. To achieve this, we use a neural network to approximate the Q function, and the function approximation problem turn into a optimization problem. We will use the mean squared error (MSE) as the loss function. The objective function is given by\n",
    "\n",
    "\\begin{equation}\n",
    "J=\\mathbb{E}\\left[\\left(R+\\gamma \\max _{a \\in \\mathcal{A}\\left(S^{\\prime}\\right)} \\hat{q}\\left(S^{\\prime}, a, w\\right)-\\hat{q}(S, A, w)\\right)^2\\right]\n",
    "\\end{equation}\n",
    "\n",
    "Using gradient descent, we can update the weights of the neural network of Q function by\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_w J=\\mathbb{E}\\left[\\left(R+\\gamma \\max_{a \\in \\mathcal{A}\\left(S^{\\prime}\\right)} \\hat{q}\\left(S^{\\prime}, a, w\\right)-\\hat{q}(S, A, w)\\right) \\nabla_w \\hat{q}(S, A, w)\\right]\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "w_{t+1} \\gets w_t - \\alpha_t \\left(r_{t+1}+\\gamma \\max_{a^{\\prime} \\in \\mathcal{A}(s_{t+1})} \\hat{q}\\left(s_{t+1}, a^{\\prime}, w_t\\right)-\\hat{q}(s_t, a_t, w_t)\\right) \\nabla_{w_t} \\hat{q}(s_t, a_t, w_t)\n",
    "\\end{equation}\n",
    "\n",
    "Thanks to the *autogradient* feature of **PyTorch**, we can easily implement the above progress (i.e., solving the optimization problem).\n",
    "\n",
    "Finally, we update the policy using the learnt Q function. Note that the exploitation part in such $\\varepsilon$-greedy policy can be act as the target policy.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\pi_{t+1}\\left(a \\mid s_t\\right)=1-\\frac{\\varepsilon}{|\\mathcal{A}(s)|}(|\\mathcal{A}(s)|-1) \\text { if } a=\\arg \\max _{a \\in \\mathcal{A}\\left(s_t\\right)} \\hat{q}\\left(s_t, a, w_{t+1}\\right) \\\\\n",
    "& \\pi_{t+1}\\left(a \\mid s_t\\right)=\\frac{\\varepsilon}{|\\mathcal{A}(s)|} \\text { otherwise }\n",
    "\\end{aligned}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNN_Agent():\n",
    "    \"\"\" Since the discrete actions have been redefined as {0,1,2,3} by using the wapper file, we can simply represent the action by a number. \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 Q_func:torch.nn.Module,\n",
    "                 action_dim:int,\n",
    "                 optimizer:torch.optim.Optimizer,\n",
    "                 epsilon:float = 0.1,\n",
    "                 gamma:float = 0.9,\n",
    "                 device:torch.device = torch.device(\"cpu\")\n",
    "                 ) -> None:\n",
    "        self.device = device\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.Q_func = Q_func\n",
    "        self.criteria = torch.nn.MSELoss()\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def get_target_action(self,obs:np.ndarray) -> int:\n",
    "        \"\"\"The input of nn must be a tensor. \n",
    "            Here, the input data of the agent is numpy arrays, so we need to convert it to tensor first \"\"\"\n",
    "        obs = torch.tensor(obs,dtype=torch.float32).to(self.device)\n",
    "        Q_list = self.Q_func(obs)\n",
    "        \n",
    "        \"\"\"The output of nn is a tensor, so we need to convert it to numpy array and then to int type\"\"\"\n",
    "        action = torch.argmax(Q_list).item()\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def get_behavior_action(self,obs:np.ndarray) -> int:\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = np.random.choice(self.action_dim)\n",
    "        else:\n",
    "            action = self.get_target_action(obs)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def Q_star_approximation(self,\n",
    "                             obs:np.ndarray,\n",
    "                             action:int,\n",
    "                             reward:float,\n",
    "                             next_obs:np.ndarray,\n",
    "                             terminated:bool) -> None:\n",
    "        \n",
    "        obs = torch.tensor(obs,dtype=torch.float32).to(self.device)\n",
    "        next_obs = torch.tensor(next_obs,dtype=torch.float32).to(self.device)\n",
    "        current_Q = self.Q_func(obs)[action]\n",
    "        TD_target = reward + (1-float(terminated)) * self.gamma * self.Q_func(next_obs).max()\n",
    "        loss = self.criteria(current_Q,TD_target)\n",
    "        # Now, we directly use gradient descent to optimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use a simple MLP with 2 hidden layers to approximate the Q function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Network(torch.nn.Module):\n",
    "    \"\"\"You can define your own network structure here.\"\"\"\n",
    "    def __init__(self,obs_dim:int,action_dim) -> None:\n",
    "        super(Q_Network,self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(obs_dim,64)\n",
    "        self.fc2 = torch.nn.Linear(64,64)\n",
    "        self.fc3 = torch.nn.Linear(64,action_dim)\n",
    "            \n",
    "    def forward(self,x:torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainManager():\n",
    "    def __init__(self,\n",
    "                 env:gym.Env,\n",
    "                 episode_num:int = 1000,\n",
    "                 lr:float = 0.001,\n",
    "                 gamma:float = 0.9,\n",
    "                 epsilon:float = 0.1) -> None:\n",
    "        \n",
    "        \"\"\"The device is automatically selected according to the availability of GPU\"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \"\"\"if you want to use CPU, you can use the code below\"\"\"\n",
    "        # self.device = torch.device(\"cpu\")\n",
    "        \n",
    "        self.env = env\n",
    "        self.episode_num = episode_num\n",
    "        obs_dim = gym.spaces.utils.flatdim(env.observation_space) \n",
    "        action_dim = env.action_space.n \n",
    "        Q_func = Q_Network(obs_dim,action_dim).to(self.device)\n",
    "        optimizer = torch.optim.Adam(Q_func.parameters(),lr=lr)\n",
    "        self.agent = QNN_Agent(Q_func = Q_func,\n",
    "                               action_dim = action_dim,\n",
    "                               optimizer = optimizer,\n",
    "                               epsilon = epsilon,\n",
    "                               gamma = gamma,\n",
    "                               device = self.device)\n",
    "    \n",
    "    def train_episode(self,is_render:bool=False) -> float:\n",
    "        total_reward = 0 \n",
    "        obs,_ = self.env.reset() \n",
    "        while True:\n",
    "            action = self.agent.get_behavior_action(obs) \n",
    "            next_obs, reward, terminated, _, _ = self.env.step(action) \n",
    "            total_reward += reward \n",
    "            self.agent.Q_star_approximation(obs,action,reward,next_obs,terminated)\n",
    "            obs = next_obs\n",
    "            if is_render:\n",
    "                self.env.render()\n",
    "                                \n",
    "            if terminated:\n",
    "                break\n",
    "            \n",
    "        return total_reward       \n",
    "\n",
    "    def test_episode(self) -> float:\n",
    "        total_reward = 0 \n",
    "        obs,_ = self.env.reset() \n",
    "        while True:\n",
    "            action = self.agent.get_target_action(obs) \n",
    "            next_obs, reward, terminated, _, _= self.env.step(action) \n",
    "            obs = next_obs\n",
    "            total_reward += reward\n",
    "            self.env.render()\n",
    "            if terminated: break\n",
    "            \n",
    "        return total_reward\n",
    "            \n",
    "    def train(self) -> None:     \n",
    "        for e in range(self.episode_num):\n",
    "            episode_reward = self.train_episode()\n",
    "            print('Episode %s: Total Reward = %.2f'%(e,episode_reward)) \n",
    "            \n",
    "            if e%100 == 0: \n",
    "                test_reward = self.test_episode()\n",
    "                print('Test Total Reward = %.2f'%(test_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v1')\n",
    "    Manger = TrainManager(env = env,\n",
    "                        episode_num = 1000,\n",
    "                        lr = 0.001,\n",
    "                        gamma = 0.9,\n",
    "                        epsilon = 0.1\n",
    "                        )\n",
    "    Manger.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
