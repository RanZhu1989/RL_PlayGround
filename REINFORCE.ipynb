{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a simple implementation of the REINFORCE algorithm. REINFORCE is a basic policy gradient algorithm which is on-policy. For such a policy gradient algorithm, the objective function is the average one-step reward:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "J\\doteq\\bar{r}_\\pi & \\doteq d_\\pi^T r_\\pi \\\\\n",
    "& =\\sum_s d_\\pi(s) r_\\pi(s) \\\\\n",
    "& =\\mathbb{E}\\left[r_\\pi(S)\\right]\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Our goal is to solve an optimization problem to maximize the objective function $J$ and obtain the optimal policy $\\pi^*$. Here, the policy will be a parameterized function $\\pi(a \\mid s, \\theta)$, where $\\theta$ is the parameter of the policy function. The gradient of the objective function can be **approximated** as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_\\theta J \\approx \\mathbb{E}\\left[\\nabla_\\theta \\ln \\pi(A \\mid S, \\theta) q_\\pi(S, A)\\right]\n",
    "\\end{equation}\n",
    "As the discount $\\gamma$ approaches 1, the approximation becomes more accurate. When $\\gamma=1$, both sides of the equation are equal.\n",
    "\n",
    "One can use the sothastic gradient ascent to update the policy parameter $\\theta$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_{t+1}=\\theta_t+\\alpha \\nabla_\\theta \\ln \\pi\\left(a_t \\mid s_t, \\theta_t\\right) q_t\\left(s_t, a_t\\right)\n",
    "\\end{equation}\n",
    "\n",
    "The above procedure constructs the *\"main loop\"* of classical policy gradient algorithms. From the structure of the equation, we can treat the term $\\ln \\pi\\left(a_t \\mid s_t, \\theta_t\\right) q_t\\left(s_t, a_t\\right)$ as the **approximated** objective function of the optimization problem. Such objective function is actually the *loss function* for training the policy network. Therefore, the policy network can be trained by **maximize** the loss function:\n",
    "\n",
    "\\begin{equation}\n",
    "\"loss\" = \\ln \\pi\\left(a_t \\mid s_t, \\theta_t\\right) q_t\\left(s_t, a_t\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Again, we should remember the \"policy training\" is actually solving an optimization problem but not to approximate something. Hence, we use \"loss\" in the above equation. Interestingly, we can use any nonlinear solver to solve the problem such as *IPOPT*, *SNOPT* and even *heuristics* such as genetic algorithm. However, the most common method in RL is to use the gradient descent method. In this notebook, we will continue to use feature of automatic differentiation in PyTorch to calculate the gradient of the loss function and update the policy network.\n",
    "\n",
    "Now the question is how to calculate the current action value $q_t(s_t,a_t)$. Similar to the MC Algorithm, REINFORCE uses the Monte Carlo method to estimate the action value since $q_t(s_t,a_t) = \\mathbb{E} \\left[G_t \\mid S_t=s_t, A_t=a_t\\right]$. Therefore, the learning progress (i.e., parameters updating) only comes after the end of an episode and lead to a off-line learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE_Agent():\n",
    "    \"\"\" Since the discrete actions have been redefined as {0,1,2,3} by using the wapper file, we can simply represent the action by a number. \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 policy_network:torch.nn, # The policy network\n",
    "                 episode_recorder:object, # The episode recorder\n",
    "                 optimizer:torch.optim, \n",
    "                 gamma:float = 0.9,\n",
    "                 device:torch.device = torch.device(\"cpu\")\n",
    "                 ) -> None:\n",
    "        \n",
    "        self.device = device\n",
    "        self.policy_network = policy_network\n",
    "        self.episode_recorder = episode_recorder\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.gamma = gamma\n",
    "            \n",
    "    def get_action(self, obs:np.ndarray) -> int:\n",
    "        \"\"\" The output of policy network is the action distribution. \n",
    "            Apart from the sampled action, the sampled action distribution is used for training the policy network.\"\"\"\n",
    "        \n",
    "        obs = torch.tensor(obs, dtype = torch.float32).to(self.device)\n",
    "        picked_action = torch.distributions.Categorical(self.policy_network(obs)).sample()  \n",
    "        picked_action_prob = self.policy_network(obs).gather(0, picked_action)\n",
    "        log_prob = torch.log(picked_action_prob)\n",
    "               \n",
    "        return picked_action.item(), log_prob\n",
    "    \n",
    "    def calculate_discounted_return(self, episode_reward:torch.tensor) -> torch.tensor:\n",
    "        \"\"\"G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... + \\gamma^{T-t-1} R_{T} \"\"\"\n",
    "        \n",
    "        length = len(episode_reward)\n",
    "        G = torch.zeros_like(episode_reward)\n",
    "        for i in range(length):\n",
    "            G[i] = sum([episode_reward[j] * self.gamma ** j for j in range(i, length)])\n",
    "\n",
    "        return G\n",
    "    \n",
    "    def episode_train(self, trajectory:list) -> None:\n",
    "        \"\"\" Train the policy network with the trajectory of one episode. \"\"\"\n",
    "        self.trajctrory = trajectory\n",
    "        trajectory_length = len(trajectory) # Get the length of the trajectory\n",
    "        episode_reward = torch.tensor([trajectory[i][0] for i in range(trajectory_length)], dtype = torch.float32).to(self.device)\n",
    "        G = self.calculate_discounted_return(episode_reward)\n",
    "        episode_log_prob = [trajectory[i][1] for i in range(trajectory_length)]\n",
    "        \"\"\" Here, we should reset the gradient of the optimizer for each episode BUT NOT for each step, calculate the gradient for each step, and then call the optimizer for a update.\n",
    "            Because the parameters of the policy network should be updated considering the whole trajectory. \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        for step in range(trajectory_length):\n",
    "            loss = -episode_log_prob[step] * G[step]\n",
    "            loss.backward()\n",
    "        self.optimizer.step()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_Network(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,obs_dim:int,action_dim:int) -> None:\n",
    "        super(Policy_Network, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(obs_dim,128)\n",
    "        self.fc2 = torch.nn.Linear(128,action_dim)\n",
    "                    \n",
    "    def forward(self,x:torch.tensor) -> torch.tensor:\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x),dim=0) # The softmax function is used to convert the output to a probability distribution\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Episode_Recorder():\n",
    "    \"\"\" Record the trajectory, i.e, Tuple(r,log_pi), of one episode. \n",
    "        pi(a|s) is the probability of taking action a in state s.)\"\"\"\n",
    "    \n",
    "    def __init__(self, device:torch.device = torch.device(\"cpu\")) -> None:\n",
    "        self.trajectory = []\n",
    "        self.device = device\n",
    "        \n",
    "    def append(self, reward:float, log_prob:torch.tensor) -> None:\n",
    "        self.trajectory.append([reward, log_prob])\n",
    "        \n",
    "    def get_trajectory(self) -> list:\n",
    "        return self.trajectory\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        self.trajectory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainManager():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 env:gym.Env,\n",
    "                 episode_num:int = 1000,\n",
    "                 lr:float = 0.001,\n",
    "                 gamma:float = 0.9\n",
    "                 ) -> None:\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # self.device = torch.device(\"cpu\")\n",
    "        \n",
    "        self.env = env\n",
    "        self.episode_num = episode_num\n",
    "        obs_dim = gym.spaces.utils.flatdim(env.observation_space) \n",
    "        action_dim = env.action_space.n\n",
    "        episode_recorder = Episode_Recorder(device = self.device)\n",
    "        policy_network = Policy_Network(obs_dim,action_dim).to(self.device)\n",
    "        optimizer = torch.optim.Adam(policy_network.parameters(),lr=lr)\n",
    "        self.agent = REINFORCE_Agent(policy_network = policy_network,\n",
    "                               episode_recorder = episode_recorder,\n",
    "                               optimizer = optimizer,\n",
    "                               gamma = gamma,\n",
    "                               device = self.device)\n",
    "        \n",
    "        self.episode_total_rewards = np.zeros(self.episode_num)\n",
    "        self.index_episode = 0    \n",
    "        \n",
    "    def train_episode(self,is_render:bool=False) -> float:\n",
    "        total_reward = 0\n",
    "        self.agent.episode_recorder.clear() \n",
    "        obs,_ = self.env.reset() \n",
    "        obs = np.array(obs)\n",
    "        while True:\n",
    "            action, log_prob = self.agent.get_action(obs) \n",
    "            next_obs, reward, terminated, _, _ = self.env.step(action)\n",
    "            self.agent.episode_recorder.append(reward, log_prob)\n",
    "            total_reward += reward \n",
    "            next_obs = np.array(next_obs)\n",
    "            obs = next_obs\n",
    "            if is_render:\n",
    "                self.env.render()\n",
    "                                \n",
    "            if terminated:\n",
    "                self.episode_total_rewards[self.index_episode] = total_reward\n",
    "                self.index_episode += 1\n",
    "                break\n",
    "            \n",
    "        trajectory = self.agent.episode_recorder.get_trajectory() # Get the trajectory of the episode for plotting\n",
    "        self.agent.episode_train(trajectory)\n",
    "            \n",
    "        return total_reward  \n",
    "    \n",
    "    def test_episode(self) -> float:\n",
    "        total_reward = 0 \n",
    "        obs,_ = self.env.reset() \n",
    "        obs = np.array(obs)\n",
    "        while True:\n",
    "            action,_ = self.agent.get_action(obs) \n",
    "            next_obs, reward, terminated, _, _= self.env.step(action) \n",
    "            obs = np.array(next_obs)\n",
    "            total_reward += reward\n",
    "            self.env.render()\n",
    "            if terminated: break\n",
    "            \n",
    "        return total_reward\n",
    "    \n",
    "    def train(self) -> None:      \n",
    "        for e in range(self.episode_num):\n",
    "            episode_reward = self.train_episode()\n",
    "            print('Episode %s: Total Reward = %.2f'%(e,episode_reward)) \n",
    "            \n",
    "            \"\"\"Here we can add the test function.\"\"\"\n",
    "            # if e % 100 == 0: \n",
    "            #     test_reward = self.test_episode()\n",
    "            #     print('Test Total Reward = %.2f'%(test_reward))\n",
    "            \n",
    "    def plotting(self,smoothing_window:int = 10) -> None:\n",
    "        \"\"\" Plot the episode reward over time. \"\"\"\n",
    "        fig = plt.figure(figsize=(10,5))\n",
    "        plt.plot(self.episode_total_rewards,label=\"Episode Reward\")\n",
    "        # Use rolling mean to smooth the curve\n",
    "        rewards_smoothed = pd.Series(self.episode_total_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
    "        plt.plot(rewards_smoothed,label=\"Episode Reward (Smoothed)\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Episode Reward')\n",
    "        plt.title(\"Episode Reward over Time\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v1')\n",
    "    Manger = TrainManager(env = env,\n",
    "                        episode_num = 1000,\n",
    "                        lr = 0.001,\n",
    "                        gamma = 0.98\n",
    "                        )\n",
    "    Manger.train()\n",
    "    Manger.plotting()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
