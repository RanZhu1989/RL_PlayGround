{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch \n",
    "import collections\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Agent():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 Q_func,\n",
    "                 action_size,\n",
    "                 optimizer,\n",
    "                 replay_buffer,\n",
    "                 replay_start_size,\n",
    "                 batch_size,\n",
    "                 replay_frequent,\n",
    "                 target_sync_frequent,\n",
    "                 epsilon=0.1,\n",
    "                 gamma=0.9,\n",
    "                 device='cpu'):\n",
    "        self.device = device\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.exp_counter = 0\n",
    "        \n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.replay_start_size = replay_start_size\n",
    "        self.batch_size = batch_size\n",
    "        self.replay_frequent = replay_frequent\n",
    "        \n",
    "        self.target_update_frequent = target_sync_frequent\n",
    "        \n",
    "        self.main_Q_func = Q_func\n",
    "        self.target_Q_func = copy.deepcopy(Q_func)\n",
    "        \n",
    "        self.criteria = torch.nn.MSELoss()\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    pass\n",
    "\n",
    "    def get_target_action(self,obs):\n",
    "        # action of determine target policy by choosing the action with the highest Q value. This method is used for testing.\n",
    "        obs = torch.tensor(obs,dtype=torch.float32,device=self.device)\n",
    "        # obs = torch.FloatTensor(obs)\n",
    "        Q_list = self.target_Q_func(obs)\n",
    "        action = int(torch.argmax(Q_list).clone().detach().cpu().numpy())\n",
    "        return action\n",
    "\n",
    "    def get_behavior_action(self,obs):\n",
    "        # For such an off-policy algorithm, we just modified an epsilon-greedy policy from the target one for exploration.\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = np.random.choice(self.action_size)\n",
    "        else:\n",
    "            action = self.get_target_action(obs)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def sync_target_Q_func(self):\n",
    "        for target_params, main_params in zip(self.target_Q_func.parameters(), self.main_Q_func.parameters()):\n",
    "            target_params.data.copy_(main_params.data)\n",
    "            \n",
    "    \n",
    "    def batch_Q_approximation(self,batch_obs,batch_action,batch_reward,batch_next_obs,batch_terminated):\n",
    "        # Here we use a batch of data to calculate current Q value. \n",
    "        # Different from the single (S,A,R,S') tuple that we use Q_func(obs)[action], each of the batch data has a Q value for different actions.\n",
    "        # Therefore, we use torch.gather to get the Q value of the action that we actually take.\n",
    "    \n",
    "        batch_current_Q = torch.gather(self.main_Q_func(batch_obs),1,batch_action).squeeze(1)\n",
    "        \n",
    "        # Note that if terminated is True, there will be no next_state and next_action. In this case, the target_Q is just reward\n",
    "        batch_TD_target = batch_reward + (1-batch_terminated) * self.gamma * self.target_Q_func(batch_next_obs).max(1)[0]\n",
    "        loss = self.criteria(batch_current_Q,batch_TD_target)\n",
    "    \n",
    "        # Here, we directly use gradient descent to optimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "            \n",
    "    def Q_approximation(self,obs,action,reward,next_obs,terminated):\n",
    "        self.exp_counter += 1\n",
    "        self.replay_buffer.append((obs,action,reward,next_obs,terminated))\n",
    "        if self.exp_counter%self.target_update_frequent == 0:\n",
    "            self.sync_target_Q_func()\n",
    "        \n",
    "        if len(self.replay_buffer) > self.replay_start_size and self.exp_counter%self.replay_frequent == 0:\n",
    "            self.batch_Q_approximation(*self.replay_buffer.sample(self.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Network(torch.nn.Module):\n",
    "    def __init__(self,obs_size,action_size):\n",
    "        super(Q_Network,self).__init__()\n",
    "        self.network = self.mlp_network(obs_size,action_size)\n",
    "        \n",
    "    def mlp_network(self,obs_size,action_size):\n",
    "        mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(obs_size,64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64,64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64,action_size)\n",
    "        )\n",
    "        return mlp\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self,capacity,device=\"cpu\"):\n",
    "        self.device = device\n",
    "        # Here, we use deque to implement the replay buffer. \n",
    "        # Collections.deque is a double-ended queue, which supports adding and removing elements from either end.\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "        \n",
    "    def append(self,exp_data):\n",
    "        self.buffer.append(exp_data)\n",
    "        \n",
    "    def sample(self,batch_size):\n",
    "        # Here, we use random.sample to randomly select a batch of experiences from the replay buffer.\n",
    "        # Note that the return type of random.sample is a list, so we need to convert it to a numpy array.\n",
    "        mini_batch = random.sample(self.buffer,batch_size)\n",
    "        obs_batch, action_batch, reward_batch, next_obs_batch, terminated_batch = zip(*mini_batch)\n",
    "        \n",
    "        obs_batch = np.array(obs_batch)\n",
    "        next_obs_batch = np.array(next_obs_batch)\n",
    "        \n",
    "        obs_batch = torch.tensor(obs_batch,dtype=torch.float32,device=self.device)\n",
    "        \n",
    "        action_batch = torch.tensor(action_batch,dtype=torch.int64,device=self.device) # for gather function, the index should be int type\n",
    "        action_batch = action_batch.unsqueeze(1)\n",
    "        \n",
    "        reward_batch = torch.tensor(reward_batch,dtype=torch.float32,device=self.device)\n",
    "        next_obs_batch = torch.tensor(next_obs_batch,dtype=torch.float32,device=self.device)\n",
    "        terminated_batch = torch.tensor(terminated_batch,dtype=torch.int64,device=self.device)\n",
    "          \n",
    "        return obs_batch, action_batch, reward_batch, next_obs_batch, terminated_batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainManager():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 env,\n",
    "                 episode_num = 1000,\n",
    "                 lr = 0.001,\n",
    "                 gamma = 0.9,\n",
    "                 epsilon = 0.1,\n",
    "                 buffer_capacity = 2000,\n",
    "                 replay_start_size = 200,\n",
    "                 replay_frequent = 4,\n",
    "                 target_sync_frequent = 200,\n",
    "                 batch_size = 32):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # self.device = 'cpu'\n",
    "        self.env = env\n",
    "        self.episode_num = episode_num\n",
    "        obs_size = gym.spaces.utils.flatdim(env.observation_space)\n",
    "        action_size = env.action_space.n\n",
    "        self.buffer = ReplayBuffer(capacity=buffer_capacity,device=self.device)\n",
    "        Q_func = Q_Network(obs_size,action_size)\n",
    "        Q_func.to(self.device)\n",
    "        optimizer = torch.optim.Adam(Q_func.parameters(),lr=lr)\n",
    "        self.agent = DQN_Agent(Q_func = Q_func,\n",
    "                               action_size = action_size,\n",
    "                               optimizer = optimizer,\n",
    "                               replay_buffer = self.buffer,\n",
    "                               replay_start_size = replay_start_size,\n",
    "                               batch_size = batch_size,\n",
    "                               replay_frequent = replay_frequent,\n",
    "                               target_sync_frequent = target_sync_frequent,\n",
    "                               epsilon = epsilon,\n",
    "                               gamma = gamma,\n",
    "                               device = self.device)\n",
    "        \n",
    "        \n",
    "    def train_episode(self,is_render=False):\n",
    "        total_reward = 0 # record total reward in one episode\n",
    "        obs,_ = self.env.reset() # reset env and get initial state\n",
    "        obs = np.array(obs)\n",
    "        while True:\n",
    "            action = self.agent.get_behavior_action(obs) # get action using learned epsilon-greedy policy\n",
    "            next_obs, reward, terminated, _, _ = self.env.step(action) # take action and get next_state, reward, done, info\n",
    "            total_reward += reward \n",
    "            next_obs = np.array(next_obs)\n",
    "            self.agent.Q_approximation(obs,action,reward,next_obs,terminated)\n",
    "            obs = next_obs\n",
    "            if is_render:\n",
    "                self.env.render()\n",
    "                                \n",
    "            if terminated:\n",
    "                break\n",
    "            \n",
    "        return total_reward       \n",
    "\n",
    "    def test_episode(self):\n",
    "        total_reward = 0 # record total reward in one episode\n",
    "        obs,_ = self.env.reset() # reset env and get initial state\n",
    "        obs = np.array(obs)\n",
    "        while True:\n",
    "            action = self.agent.get_target_action(obs) # get action using target policy\n",
    "            next_obs, reward, terminated, _, _= self.env.step(action) # take action and get next_state, reward, done, info\n",
    "            obs = np.array(next_obs)\n",
    "            total_reward += reward\n",
    "            self.env.render()\n",
    "            if terminated: break\n",
    "            \n",
    "        return total_reward\n",
    "            \n",
    "            \n",
    "    def train(self):        \n",
    "        for e in range(self.episode_num):\n",
    "            episode_reward = self.train_episode()\n",
    "            print('Episode %s: Total Reward = %.2f'%(e,episode_reward)) \n",
    "            \n",
    "            if e%100 == 0: \n",
    "                test_reward = self.test_episode()\n",
    "                print('Test Total Reward = %.2f'%(test_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v1')\n",
    "    Manger = TrainManager(env = env,\n",
    "                        episode_num = 1000,\n",
    "                        lr = 0.001,\n",
    "                        gamma = 0.9,\n",
    "                        epsilon = 0.1,\n",
    "                        target_sync_frequent = 200,\n",
    "                        )\n",
    "    Manger.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
