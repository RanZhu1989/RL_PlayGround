{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch \n",
    "import collections\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *naive* Q-learning with Q function approximation (Q_learning_NN.ipynb) shows a poor performance. One of the reasons is related to the sampling part. Since the continuous collected samples are highly correlated, the estimated expectation could be biased. To solve this problem, we can use the experience replay buffer. The experience replay buffer stores the samples in a buffer and randomly sample from the buffer to train the network. This can reduce the correlation between the samples.\n",
    "\n",
    "We can use the same network structure as the naive Q-learning with Q function approximation. The only difference is that we use the experience replay buffer to store the samples and randomly sample from the buffer to train the network. The training process uses the samples from the experience replay buffer to update the network parameters. \n",
    "\\begin{equation}\n",
    "w_{t+1} \\gets w_t - \\alpha_t \\left(r_{t+1}+\\gamma \\max_{a^{\\prime} \\in \\mathcal{A}(s_{t+1})} \\hat{q}\\left(s_{t+1}, a^{\\prime}, w_t\\right)-\\hat{q}(s_t, a_t, w_t)\\right) \\nabla_{w_t} \\hat{q}(s_t, a_t, w_t)\n",
    "\\end{equation}\n",
    "where \\{$s_t$, $a_t$, $r_t$, $s_{t+1}$\\} is uniformly sampled from the experience replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Agent():\n",
    "    \n",
    "    \"\"\" Since the discrete actions have been redefined as {0,1,2,3} by using the wapper file, we can simply represent the action by a number. \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 Q_func,\n",
    "                 action_size,\n",
    "                 optimizer,\n",
    "                 replay_buffer, # Object of ReplayBuffer\n",
    "                 replay_start_size, # The number of experiences stored in the replay buffer before learning starts\n",
    "                 batch_size, # The number of experiences to sample from the replay buffer for every learning iteration\n",
    "                 replay_frequent, # Train the network every {replay_frequent} steps, which would also help to decorrelate the samples\n",
    "                 epsilon=0.1,\n",
    "                 gamma=0.9,\n",
    "                 device='cpu'):\n",
    "        self.device = device\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.replay_start_size = replay_start_size\n",
    "        self.batch_size = batch_size\n",
    "        self.replay_frequent = replay_frequent\n",
    "        \n",
    "        \"\"\"Here, we set a Global Counter for **interactions with the environment**, which is used to determine when to update the target network.\"\"\"\n",
    "        self.exp_counter = 0 \n",
    "        \n",
    "        self.Q_func = Q_func\n",
    "        self.criteria = torch.nn.MSELoss()\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    pass\n",
    "\n",
    "    def get_target_action(self,obs):\n",
    "        obs = torch.tensor(obs,dtype=torch.float32,device=self.device)\n",
    "        Q_list = self.Q_func(obs)\n",
    "        action = int(torch.argmax(Q_list).clone().detach().cpu().numpy())\n",
    "        return action\n",
    "\n",
    "    def get_behavior_action(self,obs):\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = np.random.choice(self.action_size)\n",
    "        else:\n",
    "            action = self.get_target_action(obs)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def batch_Q_approximation(self,batch_obs,batch_action,batch_reward,batch_next_obs,batch_terminated):\n",
    "        \"\"\"\"\"\"\n",
    "        batch_current_Q = torch.gather(self.Q_func(batch_obs),1,batch_action).squeeze(1)\n",
    "        batch_TD_target = batch_reward + (1-batch_terminated) * self.gamma * self.Q_func(batch_next_obs).max(1)[0]\n",
    "        loss = self.criteria(batch_current_Q,batch_TD_target)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "            \n",
    "    def Q_approximation(self,obs,action,reward,next_obs,terminated):\n",
    "        \"\"\"After each interaction with the environment, we call this function and check whether the replay buffer has enough experiences to start learning.\"\"\"\n",
    "        self.exp_counter += 1 # Update the Global Counter here, since we defined the counter in Agent class.\n",
    "        self.replay_buffer.append((obs,action,reward,next_obs,terminated)) # Store the experience in the replay buffer\n",
    "        \n",
    "        # Start learning after the replay buffer has enough experiences\n",
    "        if len(self.replay_buffer) > self.replay_start_size and self.exp_counter%self.replay_frequent == 0: \n",
    "            self.batch_Q_approximation(*self.replay_buffer.sample(self.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Network(torch.nn.Module):\n",
    "    def __init__(self,obs_size,action_size):\n",
    "        super(Q_Network,self).__init__()\n",
    "        self.network = self.mlp_network(obs_size,action_size)\n",
    "        \n",
    "    def mlp_network(self,obs_size,action_size):\n",
    "        mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(obs_size,64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64,64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64,action_size)\n",
    "        )\n",
    "        return mlp\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use python build-in libratory collections to implement the experience replay buffer. The deque is a double-ended queue. It can be used to add or remove elements from both ends. More details can be found in https://docs.python.org/2/library/collections.html#collections.deque. \n",
    "\n",
    "In this code, the replay will start after collecting certain number of experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self,capacity,device=\"cpu\"):\n",
    "        \"\"\"Capacity is the maximum number of experiences that can be stored in the replay buffer. \n",
    "            If the number of experiences exceeds the capacity, the oldest experiences will be removed.\"\"\"\n",
    "        self.device = device\n",
    "        \"\"\"Here, we use deque to implement the replay buffer. \n",
    "            Collections.deque is a double-ended queue, which supports adding and removing elements from either end.\"\"\"\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "        \n",
    "    def append(self,exp_data):\n",
    "        self.buffer.append(exp_data)\n",
    "        \n",
    "    def sample(self,batch_size):\n",
    "        \"\"\"Here, we use random.sample to randomly select a batch of experiences from the replay buffer.\n",
    "            Note that the return type of random.sample is a list, so we need to convert it to a numpy array.\"\"\"\n",
    "        mini_batch = random.sample(self.buffer,batch_size)\n",
    "        obs_batch, action_batch, reward_batch, next_obs_batch, terminated_batch = zip(*mini_batch)\n",
    "        \n",
    "        obs_batch = np.array(obs_batch)\n",
    "        next_obs_batch = np.array(next_obs_batch)\n",
    "        \n",
    "        obs_batch = torch.tensor(obs_batch,dtype=torch.float32,device=self.device)\n",
    "        \n",
    "        action_batch = torch.tensor(action_batch,dtype=torch.int64,device=self.device) # for gather function, the index should be int type\n",
    "        action_batch = action_batch.unsqueeze(1)\n",
    "        \n",
    "        reward_batch = torch.tensor(reward_batch,dtype=torch.float32,device=self.device)\n",
    "        next_obs_batch = torch.tensor(next_obs_batch,dtype=torch.float32,device=self.device)\n",
    "        terminated_batch = torch.tensor(terminated_batch,dtype=torch.int64,device=self.device)\n",
    "          \n",
    "        return obs_batch, action_batch, reward_batch, next_obs_batch, terminated_batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainManager():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 env,\n",
    "                 episode_num = 1000,\n",
    "                 lr = 0.001,\n",
    "                 gamma = 0.9,\n",
    "                 epsilon = 0.1,\n",
    "                 buffer_capacity = 2000,\n",
    "                 replay_start_size = 200,\n",
    "                 replay_frequent = 4,\n",
    "                 batch_size = 32):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # self.device = 'cpu'\n",
    "        self.env = env\n",
    "        self.episode_num = episode_num\n",
    "        obs_size = gym.spaces.utils.flatdim(env.observation_space) \n",
    "        action_size = env.action_space.n\n",
    "        self.buffer = ReplayBuffer(capacity=buffer_capacity,device=self.device)\n",
    "        Q_func = Q_Network(obs_size,action_size).to(self.device)\n",
    "        optimizer = torch.optim.Adam(Q_func.parameters(),lr=lr)\n",
    "        self.agent = DQN_Agent(Q_func = Q_func,\n",
    "                               action_size = action_size,\n",
    "                               optimizer = optimizer,\n",
    "                               replay_buffer = self.buffer,\n",
    "                               replay_start_size = replay_start_size,\n",
    "                               batch_size = batch_size,\n",
    "                               replay_frequent=replay_frequent,\n",
    "                               epsilon = epsilon,\n",
    "                               gamma = gamma,\n",
    "                               device = self.device)\n",
    "        \n",
    "    def train_episode(self,is_render=False):\n",
    "        total_reward = 0 \n",
    "        obs,_ = self.env.reset() \n",
    "        obs = np.array(obs)\n",
    "        while True:\n",
    "            action = self.agent.get_behavior_action(obs) \n",
    "            next_obs, reward, terminated, _, _ = self.env.step(action) \n",
    "            total_reward += reward \n",
    "            next_obs = np.array(next_obs)\n",
    "            self.agent.Q_approximation(obs,action,reward,next_obs,terminated)\n",
    "            obs = next_obs\n",
    "            if is_render:\n",
    "                self.env.render()\n",
    "                                \n",
    "            if terminated:\n",
    "                break\n",
    "            \n",
    "        return total_reward       \n",
    "\n",
    "    def test_episode(self):\n",
    "        total_reward = 0 \n",
    "        obs,_ = self.env.reset() \n",
    "        obs = np.array(obs)\n",
    "        while True:\n",
    "            action = self.agent.get_target_action(obs) \n",
    "            next_obs, reward, terminated, _, _= self.env.step(action) \n",
    "            obs = np.array(next_obs)\n",
    "            total_reward += reward\n",
    "            self.env.render()\n",
    "            if terminated: break\n",
    "            \n",
    "        return total_reward\n",
    "            \n",
    "            \n",
    "    def train(self):        \n",
    "        for e in range(self.episode_num):\n",
    "            episode_reward = self.train_episode()\n",
    "            print('Episode %s: Total Reward = %.2f'%(e,episode_reward)) \n",
    "            \n",
    "            if e % 100 == 0: \n",
    "                test_reward = self.test_episode()\n",
    "                print('Test Total Reward = %.2f'%(test_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Total Reward = 12.00\n",
      "Test Total Reward = 10.00\n",
      "Episode 1: Total Reward = 9.00\n",
      "Episode 2: Total Reward = 9.00\n",
      "Episode 3: Total Reward = 11.00\n",
      "Episode 4: Total Reward = 9.00\n",
      "Episode 5: Total Reward = 11.00\n",
      "Episode 6: Total Reward = 9.00\n",
      "Episode 7: Total Reward = 10.00\n",
      "Episode 8: Total Reward = 8.00\n",
      "Episode 9: Total Reward = 8.00\n",
      "Episode 10: Total Reward = 13.00\n",
      "Episode 11: Total Reward = 9.00\n",
      "Episode 12: Total Reward = 10.00\n",
      "Episode 13: Total Reward = 10.00\n",
      "Episode 14: Total Reward = 10.00\n",
      "Episode 15: Total Reward = 9.00\n",
      "Episode 16: Total Reward = 10.00\n",
      "Episode 17: Total Reward = 9.00\n",
      "Episode 18: Total Reward = 8.00\n",
      "Episode 19: Total Reward = 11.00\n",
      "Episode 20: Total Reward = 9.00\n",
      "Episode 21: Total Reward = 9.00\n",
      "Episode 22: Total Reward = 8.00\n",
      "Episode 23: Total Reward = 9.00\n",
      "Episode 24: Total Reward = 9.00\n",
      "Episode 25: Total Reward = 10.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gemin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:215: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 26: Total Reward = 10.00\n",
      "Episode 27: Total Reward = 10.00\n",
      "Episode 28: Total Reward = 10.00\n",
      "Episode 29: Total Reward = 12.00\n",
      "Episode 30: Total Reward = 10.00\n",
      "Episode 31: Total Reward = 8.00\n",
      "Episode 32: Total Reward = 9.00\n",
      "Episode 33: Total Reward = 10.00\n",
      "Episode 34: Total Reward = 10.00\n",
      "Episode 35: Total Reward = 11.00\n",
      "Episode 36: Total Reward = 8.00\n",
      "Episode 37: Total Reward = 10.00\n",
      "Episode 38: Total Reward = 10.00\n",
      "Episode 39: Total Reward = 9.00\n",
      "Episode 40: Total Reward = 12.00\n",
      "Episode 41: Total Reward = 9.00\n",
      "Episode 42: Total Reward = 9.00\n",
      "Episode 43: Total Reward = 12.00\n",
      "Episode 44: Total Reward = 10.00\n",
      "Episode 45: Total Reward = 11.00\n",
      "Episode 46: Total Reward = 10.00\n",
      "Episode 47: Total Reward = 11.00\n",
      "Episode 48: Total Reward = 12.00\n",
      "Episode 49: Total Reward = 10.00\n",
      "Episode 50: Total Reward = 8.00\n",
      "Episode 51: Total Reward = 12.00\n",
      "Episode 52: Total Reward = 9.00\n",
      "Episode 53: Total Reward = 9.00\n",
      "Episode 54: Total Reward = 9.00\n",
      "Episode 55: Total Reward = 10.00\n",
      "Episode 56: Total Reward = 9.00\n",
      "Episode 57: Total Reward = 9.00\n",
      "Episode 58: Total Reward = 10.00\n",
      "Episode 59: Total Reward = 12.00\n",
      "Episode 60: Total Reward = 9.00\n",
      "Episode 61: Total Reward = 10.00\n",
      "Episode 62: Total Reward = 10.00\n",
      "Episode 63: Total Reward = 8.00\n",
      "Episode 64: Total Reward = 9.00\n",
      "Episode 65: Total Reward = 10.00\n",
      "Episode 66: Total Reward = 9.00\n",
      "Episode 67: Total Reward = 12.00\n",
      "Episode 68: Total Reward = 11.00\n",
      "Episode 69: Total Reward = 10.00\n",
      "Episode 70: Total Reward = 9.00\n",
      "Episode 71: Total Reward = 10.00\n",
      "Episode 72: Total Reward = 8.00\n",
      "Episode 73: Total Reward = 10.00\n",
      "Episode 74: Total Reward = 10.00\n",
      "Episode 75: Total Reward = 9.00\n",
      "Episode 76: Total Reward = 11.00\n",
      "Episode 77: Total Reward = 11.00\n",
      "Episode 78: Total Reward = 9.00\n",
      "Episode 79: Total Reward = 10.00\n",
      "Episode 80: Total Reward = 10.00\n",
      "Episode 81: Total Reward = 9.00\n",
      "Episode 82: Total Reward = 12.00\n",
      "Episode 83: Total Reward = 9.00\n",
      "Episode 84: Total Reward = 10.00\n",
      "Episode 85: Total Reward = 12.00\n",
      "Episode 86: Total Reward = 9.00\n",
      "Episode 87: Total Reward = 9.00\n",
      "Episode 88: Total Reward = 12.00\n",
      "Episode 89: Total Reward = 13.00\n",
      "Episode 90: Total Reward = 11.00\n",
      "Episode 91: Total Reward = 8.00\n",
      "Episode 92: Total Reward = 11.00\n",
      "Episode 93: Total Reward = 13.00\n",
      "Episode 94: Total Reward = 14.00\n",
      "Episode 95: Total Reward = 19.00\n",
      "Episode 96: Total Reward = 21.00\n",
      "Episode 97: Total Reward = 11.00\n",
      "Episode 98: Total Reward = 11.00\n",
      "Episode 99: Total Reward = 22.00\n",
      "Episode 100: Total Reward = 21.00\n",
      "Test Total Reward = 30.00\n",
      "Episode 101: Total Reward = 16.00\n",
      "Episode 102: Total Reward = 17.00\n",
      "Episode 103: Total Reward = 26.00\n",
      "Episode 104: Total Reward = 19.00\n",
      "Episode 105: Total Reward = 42.00\n",
      "Episode 106: Total Reward = 18.00\n",
      "Episode 107: Total Reward = 25.00\n",
      "Episode 108: Total Reward = 10.00\n",
      "Episode 109: Total Reward = 9.00\n",
      "Episode 110: Total Reward = 9.00\n",
      "Episode 111: Total Reward = 34.00\n",
      "Episode 112: Total Reward = 11.00\n",
      "Episode 113: Total Reward = 23.00\n",
      "Episode 114: Total Reward = 20.00\n",
      "Episode 115: Total Reward = 9.00\n",
      "Episode 116: Total Reward = 52.00\n",
      "Episode 117: Total Reward = 12.00\n",
      "Episode 118: Total Reward = 30.00\n",
      "Episode 119: Total Reward = 12.00\n",
      "Episode 120: Total Reward = 30.00\n",
      "Episode 121: Total Reward = 10.00\n",
      "Episode 122: Total Reward = 26.00\n",
      "Episode 123: Total Reward = 20.00\n",
      "Episode 124: Total Reward = 28.00\n",
      "Episode 125: Total Reward = 56.00\n",
      "Episode 126: Total Reward = 22.00\n",
      "Episode 127: Total Reward = 21.00\n",
      "Episode 128: Total Reward = 10.00\n",
      "Episode 129: Total Reward = 26.00\n",
      "Episode 130: Total Reward = 9.00\n",
      "Episode 131: Total Reward = 33.00\n",
      "Episode 132: Total Reward = 20.00\n",
      "Episode 133: Total Reward = 12.00\n",
      "Episode 134: Total Reward = 11.00\n",
      "Episode 135: Total Reward = 25.00\n",
      "Episode 136: Total Reward = 34.00\n",
      "Episode 137: Total Reward = 16.00\n",
      "Episode 138: Total Reward = 27.00\n",
      "Episode 139: Total Reward = 16.00\n",
      "Episode 140: Total Reward = 10.00\n",
      "Episode 141: Total Reward = 22.00\n",
      "Episode 142: Total Reward = 23.00\n",
      "Episode 143: Total Reward = 32.00\n",
      "Episode 144: Total Reward = 30.00\n",
      "Episode 145: Total Reward = 29.00\n",
      "Episode 146: Total Reward = 26.00\n",
      "Episode 147: Total Reward = 19.00\n",
      "Episode 148: Total Reward = 29.00\n",
      "Episode 149: Total Reward = 22.00\n",
      "Episode 150: Total Reward = 20.00\n",
      "Episode 151: Total Reward = 22.00\n",
      "Episode 152: Total Reward = 47.00\n",
      "Episode 153: Total Reward = 38.00\n",
      "Episode 154: Total Reward = 28.00\n",
      "Episode 155: Total Reward = 13.00\n",
      "Episode 156: Total Reward = 25.00\n",
      "Episode 157: Total Reward = 67.00\n",
      "Episode 158: Total Reward = 30.00\n",
      "Episode 159: Total Reward = 30.00\n",
      "Episode 160: Total Reward = 18.00\n",
      "Episode 161: Total Reward = 44.00\n",
      "Episode 162: Total Reward = 44.00\n",
      "Episode 163: Total Reward = 23.00\n",
      "Episode 164: Total Reward = 39.00\n",
      "Episode 165: Total Reward = 32.00\n",
      "Episode 166: Total Reward = 40.00\n",
      "Episode 167: Total Reward = 46.00\n",
      "Episode 168: Total Reward = 48.00\n",
      "Episode 169: Total Reward = 34.00\n",
      "Episode 170: Total Reward = 14.00\n",
      "Episode 171: Total Reward = 79.00\n",
      "Episode 172: Total Reward = 46.00\n",
      "Episode 173: Total Reward = 41.00\n",
      "Episode 174: Total Reward = 43.00\n",
      "Episode 175: Total Reward = 36.00\n",
      "Episode 176: Total Reward = 49.00\n",
      "Episode 177: Total Reward = 65.00\n",
      "Episode 178: Total Reward = 59.00\n",
      "Episode 179: Total Reward = 40.00\n",
      "Episode 180: Total Reward = 41.00\n",
      "Episode 181: Total Reward = 28.00\n",
      "Episode 182: Total Reward = 42.00\n",
      "Episode 183: Total Reward = 40.00\n",
      "Episode 184: Total Reward = 48.00\n",
      "Episode 185: Total Reward = 39.00\n",
      "Episode 186: Total Reward = 46.00\n",
      "Episode 187: Total Reward = 51.00\n",
      "Episode 188: Total Reward = 36.00\n",
      "Episode 189: Total Reward = 48.00\n",
      "Episode 190: Total Reward = 55.00\n",
      "Episode 191: Total Reward = 27.00\n",
      "Episode 192: Total Reward = 45.00\n",
      "Episode 193: Total Reward = 43.00\n",
      "Episode 194: Total Reward = 45.00\n",
      "Episode 195: Total Reward = 212.00\n",
      "Episode 196: Total Reward = 35.00\n",
      "Episode 197: Total Reward = 49.00\n",
      "Episode 198: Total Reward = 39.00\n",
      "Episode 199: Total Reward = 71.00\n",
      "Episode 200: Total Reward = 50.00\n",
      "Test Total Reward = 59.00\n",
      "Episode 201: Total Reward = 53.00\n",
      "Episode 202: Total Reward = 42.00\n",
      "Episode 203: Total Reward = 64.00\n",
      "Episode 204: Total Reward = 31.00\n",
      "Episode 205: Total Reward = 49.00\n",
      "Episode 206: Total Reward = 76.00\n",
      "Episode 207: Total Reward = 123.00\n",
      "Episode 208: Total Reward = 100.00\n",
      "Episode 209: Total Reward = 97.00\n",
      "Episode 210: Total Reward = 132.00\n",
      "Episode 211: Total Reward = 66.00\n",
      "Episode 212: Total Reward = 70.00\n",
      "Episode 213: Total Reward = 165.00\n",
      "Episode 214: Total Reward = 344.00\n",
      "Episode 215: Total Reward = 66.00\n",
      "Episode 216: Total Reward = 84.00\n",
      "Episode 217: Total Reward = 81.00\n",
      "Episode 218: Total Reward = 136.00\n",
      "Episode 219: Total Reward = 142.00\n",
      "Episode 220: Total Reward = 154.00\n",
      "Episode 221: Total Reward = 45.00\n",
      "Episode 222: Total Reward = 151.00\n",
      "Episode 223: Total Reward = 98.00\n",
      "Episode 224: Total Reward = 45.00\n",
      "Episode 225: Total Reward = 65.00\n",
      "Episode 226: Total Reward = 75.00\n",
      "Episode 227: Total Reward = 80.00\n",
      "Episode 228: Total Reward = 75.00\n",
      "Episode 229: Total Reward = 91.00\n",
      "Episode 230: Total Reward = 127.00\n",
      "Episode 231: Total Reward = 62.00\n",
      "Episode 232: Total Reward = 58.00\n",
      "Episode 233: Total Reward = 45.00\n",
      "Episode 234: Total Reward = 111.00\n",
      "Episode 235: Total Reward = 58.00\n",
      "Episode 236: Total Reward = 39.00\n",
      "Episode 237: Total Reward = 38.00\n",
      "Episode 238: Total Reward = 48.00\n",
      "Episode 239: Total Reward = 77.00\n",
      "Episode 240: Total Reward = 88.00\n",
      "Episode 241: Total Reward = 68.00\n",
      "Episode 242: Total Reward = 39.00\n",
      "Episode 243: Total Reward = 53.00\n",
      "Episode 244: Total Reward = 73.00\n",
      "Episode 245: Total Reward = 48.00\n",
      "Episode 246: Total Reward = 45.00\n",
      "Episode 247: Total Reward = 50.00\n",
      "Episode 248: Total Reward = 50.00\n",
      "Episode 249: Total Reward = 201.00\n",
      "Episode 250: Total Reward = 112.00\n",
      "Episode 251: Total Reward = 74.00\n",
      "Episode 252: Total Reward = 62.00\n",
      "Episode 253: Total Reward = 115.00\n",
      "Episode 254: Total Reward = 148.00\n",
      "Episode 255: Total Reward = 76.00\n",
      "Episode 256: Total Reward = 53.00\n",
      "Episode 257: Total Reward = 109.00\n",
      "Episode 258: Total Reward = 113.00\n",
      "Episode 259: Total Reward = 126.00\n",
      "Episode 260: Total Reward = 96.00\n",
      "Episode 261: Total Reward = 78.00\n",
      "Episode 262: Total Reward = 120.00\n",
      "Episode 263: Total Reward = 121.00\n",
      "Episode 264: Total Reward = 107.00\n",
      "Episode 265: Total Reward = 149.00\n",
      "Episode 266: Total Reward = 120.00\n",
      "Episode 267: Total Reward = 106.00\n",
      "Episode 268: Total Reward = 46.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m'\u001b[39m\u001b[39mCartPole-v1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m Manger \u001b[39m=\u001b[39m TrainManager(env \u001b[39m=\u001b[39m env,\n\u001b[0;32m      4\u001b[0m                     episode_num \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m,\n\u001b[0;32m      5\u001b[0m                     lr \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m,\n\u001b[0;32m      6\u001b[0m                     gamma \u001b[39m=\u001b[39m \u001b[39m0.9\u001b[39m,\n\u001b[0;32m      7\u001b[0m                     epsilon \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m\n\u001b[0;32m      8\u001b[0m                     )\n\u001b[1;32m----> 9\u001b[0m Manger\u001b[39m.\u001b[39;49mtrain()\n",
      "Cell \u001b[1;32mIn[11], line 69\u001b[0m, in \u001b[0;36mTrainManager.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m):        \n\u001b[0;32m     68\u001b[0m     \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode_num):\n\u001b[1;32m---> 69\u001b[0m         episode_reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_episode()\n\u001b[0;32m     70\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpisode \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: Total Reward = \u001b[39m\u001b[39m%.2f\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m(e,episode_reward)) \n\u001b[0;32m     72\u001b[0m         \u001b[39mif\u001b[39;00m e \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \n",
      "Cell \u001b[1;32mIn[11], line 38\u001b[0m, in \u001b[0;36mTrainManager.train_episode\u001b[1;34m(self, is_render)\u001b[0m\n\u001b[0;32m     36\u001b[0m obs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(obs)\n\u001b[0;32m     37\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49mget_behavior_action(obs) \n\u001b[0;32m     39\u001b[0m     next_obs, reward, terminated, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action) \n\u001b[0;32m     40\u001b[0m     total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward \n",
      "Cell \u001b[1;32mIn[8], line 46\u001b[0m, in \u001b[0;36mDQN_Agent.get_behavior_action\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     44\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_size)\n\u001b[0;32m     45\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 46\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_target_action(obs)\n\u001b[0;32m     48\u001b[0m \u001b[39mreturn\u001b[39;00m action\n",
      "Cell \u001b[1;32mIn[8], line 37\u001b[0m, in \u001b[0;36mDQN_Agent.get_target_action\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_target_action\u001b[39m(\u001b[39mself\u001b[39m,obs):\n\u001b[1;32m---> 37\u001b[0m     obs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(obs,dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32,device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m     38\u001b[0m     Q_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mQ_func(obs)\n\u001b[0;32m     39\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(torch\u001b[39m.\u001b[39margmax(Q_list)\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v1')\n",
    "    Manger = TrainManager(env = env,\n",
    "                        episode_num = 1000,\n",
    "                        lr = 0.001,\n",
    "                        gamma = 0.9,\n",
    "                        epsilon = 0.1\n",
    "                        )\n",
    "    Manger.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
