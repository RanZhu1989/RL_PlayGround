{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch \n",
    "import collections\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adopting the replay buffer, there is still a problem for calculating the gradient of the loss function. If we update and utilize the policy network at the same time, the training process will be unstable. Therefore, we need to use a target network to calculate the target value. The target network is a copy of the main network, and it is updated every specific step size. The target network, with parameters $w_T$ is used to calculate the TD target. The main network, with parameters $w$ is used to calculate the current value. Hence, the training process is as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "w_{t+1} \\gets w_t - \\alpha_t \\left(r_{t+1}+\\gamma \\max_{a^{\\prime} \\in \\mathcal{A}(s_{t+1})} \\hat{q}\\left(s_{t+1}, a^{\\prime}, w_{T,t}\\right)-\\hat{q}(s_t, a_t, w_t)\\right) \\nabla_{w_t} \\hat{q}(s_t, a_t, w_t)\n",
    "\\end{equation}\n",
    "where \\{$s_t$, $a_t$, $r_t$, $s_{t+1}$\\} is uniformly sampled from the experience replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Agent():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 Q_func,\n",
    "                 action_size,\n",
    "                 optimizer,\n",
    "                 replay_buffer,\n",
    "                 replay_start_size,\n",
    "                 batch_size,\n",
    "                 replay_frequent,\n",
    "                 target_sync_frequent, # The frequency of synchronizing the parameters of the two Q networks\n",
    "                 epsilon = 0.1, # Initial epsilon\n",
    "                 mini_epsilon = 0.01, # Minimum epsilon\n",
    "                 explore_decay_rate = 0.0001, # Decay rate of epsilon\n",
    "                 gamma = 0.9,\n",
    "                 device='cpu'):\n",
    "        \n",
    "        self.device = device\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.exp_counter = 0\n",
    "        \n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.replay_start_size = replay_start_size\n",
    "        self.batch_size = batch_size\n",
    "        self.replay_frequent = replay_frequent\n",
    "        \n",
    "        self.target_update_frequent = target_sync_frequent\n",
    "        \n",
    "        \"\"\"Two Q functions (mian_Q and target_Q) are used to stabilize the training process. \n",
    "            Since they share the same network structure, we can use copy.deepcopy to copy the main_Q to target_Q for initialization.\"\"\"\n",
    "        self.main_Q_func = Q_func\n",
    "        self.target_Q_func = copy.deepcopy(Q_func)\n",
    "        \n",
    "        self.criteria = torch.nn.MSELoss()\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.mini_epsilon = mini_epsilon\n",
    "        self.gamma = gamma\n",
    "        self.explore_decay_rate = explore_decay_rate\n",
    "        \n",
    "    pass\n",
    "\n",
    "    def get_target_action(self,obs):\n",
    "        obs = torch.tensor(obs,dtype=torch.float32,device=self.device)\n",
    "        Q_list = self.target_Q_func(obs)\n",
    "        action = int(torch.argmax(Q_list).clone().detach().cpu().numpy())\n",
    "        return action\n",
    "\n",
    "    def get_behavior_action(self,obs):\n",
    "        \"\"\"Here, a simple epsilon decay is used to balance the exploration and exploitation.\n",
    "            The epsilon is decayed from epsilon_init to mini_epsilon.\"\"\"\n",
    "        self.epsilon = max(self.mini_epsilon,self.epsilon-self.explore_decay_rate)\n",
    "        \n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = np.random.choice(self.action_size)\n",
    "        else:\n",
    "            action = self.get_target_action(obs)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    \"\"\"Here, we defined a function to synchronize the parameters of the main_Q and target_Q.\"\"\"\n",
    "    def sync_target_Q_func(self):\n",
    "        for target_params, main_params in zip(self.target_Q_func.parameters(), self.main_Q_func.parameters()):\n",
    "            target_params.data.copy_(main_params.data)\n",
    "            \n",
    "    \n",
    "    def batch_Q_approximation(self,batch_obs,batch_action,batch_reward,batch_next_obs,batch_terminated):\n",
    "        batch_current_Q = torch.gather(self.main_Q_func(batch_obs),1,batch_action).squeeze(1)\n",
    "        batch_TD_target = batch_reward + (1-batch_terminated) * self.gamma * self.target_Q_func(batch_next_obs).max(1)[0]\n",
    "        loss = self.criteria(batch_current_Q,batch_TD_target)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "            \n",
    "    def Q_approximation(self,obs,action,reward,next_obs,terminated):\n",
    "        self.exp_counter += 1\n",
    "        self.replay_buffer.append((obs,action,reward,next_obs,terminated))\n",
    "\n",
    "        if len(self.replay_buffer) > self.replay_start_size and self.exp_counter%self.replay_frequent == 0:\n",
    "            self.batch_Q_approximation(*self.replay_buffer.sample(self.batch_size))\n",
    "        \n",
    "        # Synchronize the parameters of the two Q networks every target_update_frequent steps\n",
    "        if self.exp_counter%self.target_update_frequent == 0:\n",
    "            self.sync_target_Q_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Network(torch.nn.Module):\n",
    "    def __init__(self,obs_size,action_size):\n",
    "        super(Q_Network,self).__init__()\n",
    "        self.network = self.mlp_network(obs_size,action_size)\n",
    "        \n",
    "    def mlp_network(self,obs_size,action_size):\n",
    "        mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(obs_size,64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64,64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64,action_size)\n",
    "        )\n",
    "        return mlp\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self,capacity,device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "        \n",
    "    def append(self,exp_data):\n",
    "        self.buffer.append(exp_data)\n",
    "        \n",
    "    def sample(self,batch_size):\n",
    "        mini_batch = random.sample(self.buffer,batch_size)\n",
    "        obs_batch, action_batch, reward_batch, next_obs_batch, terminated_batch = zip(*mini_batch)\n",
    "        \n",
    "        obs_batch = np.array(obs_batch)\n",
    "        next_obs_batch = np.array(next_obs_batch)\n",
    "        \n",
    "        obs_batch = torch.tensor(obs_batch,dtype=torch.float32,device=self.device)\n",
    "        \n",
    "        action_batch = torch.tensor(action_batch,dtype=torch.int64,device=self.device) \n",
    "        action_batch = action_batch.unsqueeze(1)\n",
    "        \n",
    "        reward_batch = torch.tensor(reward_batch,dtype=torch.float32,device=self.device)\n",
    "        next_obs_batch = torch.tensor(next_obs_batch,dtype=torch.float32,device=self.device)\n",
    "        terminated_batch = torch.tensor(terminated_batch,dtype=torch.int64,device=self.device)\n",
    "          \n",
    "        return obs_batch, action_batch, reward_batch, next_obs_batch, terminated_batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainManager():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 env,\n",
    "                 episode_num = 1000,\n",
    "                 lr = 0.001,\n",
    "                 gamma = 0.9,\n",
    "                 epsilon = 0.1,\n",
    "                 mini_epsilon=0.01,\n",
    "                 explore_decay_rate=0.0001,\n",
    "                 buffer_capacity = 2000,\n",
    "                 replay_start_size = 200,\n",
    "                 replay_frequent = 4,\n",
    "                 target_sync_frequent = 200,\n",
    "                 batch_size = 32):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # self.device = 'cpu'\n",
    "        self.env = env\n",
    "        self.episode_num = episode_num\n",
    "        obs_size = gym.spaces.utils.flatdim(env.observation_space)\n",
    "        action_size = env.action_space.n\n",
    "        self.buffer = ReplayBuffer(capacity=buffer_capacity,device=self.device)\n",
    "        Q_func = Q_Network(obs_size,action_size)\n",
    "        Q_func.to(self.device)\n",
    "        optimizer = torch.optim.Adam(Q_func.parameters(),lr=lr)\n",
    "        self.agent = DQN_Agent(Q_func = Q_func,\n",
    "                               action_size = action_size,\n",
    "                               optimizer = optimizer,\n",
    "                               replay_buffer = self.buffer,\n",
    "                               replay_start_size = replay_start_size,\n",
    "                               batch_size = batch_size,\n",
    "                               replay_frequent = replay_frequent,\n",
    "                               target_sync_frequent = target_sync_frequent,\n",
    "                               epsilon = epsilon,\n",
    "                               mini_epsilon = mini_epsilon,\n",
    "                               explore_decay_rate = explore_decay_rate,\n",
    "                               gamma = gamma,\n",
    "                               device = self.device)\n",
    "        \n",
    "        \n",
    "    def train_episode(self,is_render=False):\n",
    "        total_reward = 0 \n",
    "        obs,_ = self.env.reset() \n",
    "        obs = np.array(obs)\n",
    "        while True:\n",
    "            action = self.agent.get_behavior_action(obs) \n",
    "            next_obs, reward, terminated, _, _ = self.env.step(action) \n",
    "            total_reward += reward \n",
    "            next_obs = np.array(next_obs)\n",
    "            self.agent.Q_approximation(obs,action,reward,next_obs,terminated)\n",
    "            obs = next_obs\n",
    "            if is_render:\n",
    "                self.env.render()\n",
    "                                \n",
    "            if terminated:\n",
    "                break\n",
    "            \n",
    "        return total_reward       \n",
    "\n",
    "    def test_episode(self):\n",
    "        total_reward = 0 \n",
    "        obs,_ = self.env.reset() \n",
    "        obs = np.array(obs)\n",
    "        while True:\n",
    "            action = self.agent.get_target_action(obs) \n",
    "            next_obs, reward, terminated, _, _= self.env.step(action) \n",
    "            obs = np.array(next_obs)\n",
    "            total_reward += reward\n",
    "            self.env.render()\n",
    "            if terminated: break\n",
    "            \n",
    "        return total_reward\n",
    "            \n",
    "            \n",
    "    def train(self):        \n",
    "        for e in range(self.episode_num):\n",
    "            episode_reward = self.train_episode()\n",
    "            print('Episode %s: Total Reward = %.2f'%(e,episode_reward)) \n",
    "            \n",
    "            if e%100 == 0: \n",
    "                test_reward = self.test_episode()\n",
    "                print('Test Total Reward = %.2f'%(test_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v1')\n",
    "    Manger = TrainManager(env = env,\n",
    "                        episode_num = 1000,\n",
    "                        lr = 0.001,\n",
    "                        gamma = 0.9,\n",
    "                        epsilon = 0.3,\n",
    "                        target_sync_frequent = 200,\n",
    "                        mini_epsilon = 0.01,\n",
    "                        explore_decay_rate = 0.0001\n",
    "                        )\n",
    "    Manger.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
