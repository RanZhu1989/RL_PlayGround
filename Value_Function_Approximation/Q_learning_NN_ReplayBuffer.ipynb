{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "import random\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *naive* Q-learning with Q function approximation (Q_learning_NN.ipynb) shows a poor performance. One of the reasons is related to the sampling part. Since the continuous collected samples are highly correlated, the estimated expectation could be biased. To solve this problem, we can use the experience replay buffer. The experience replay buffer stores the samples in a buffer and randomly sample from the buffer to train the network. This can reduce the correlation between the samples.\n",
    "\n",
    "We can use the same network structure as the naive Q-learning with Q function approximation. The only difference is that we use the experience replay buffer to store the samples and randomly sample from the buffer to train the network. The training process uses the samples from the experience replay buffer to update the network parameters. \n",
    "\\begin{equation}\n",
    "w_{t+1} \\gets w_t - \\alpha_t \\left(r_{t+1}+\\gamma \\max_{a^{\\prime} \\in \\mathcal{A}(s_{t+1})} \\hat{q}\\left(s_{t+1}, a^{\\prime}, w_t\\right)-\\hat{q}(s_t, a_t, w_t)\\right) \\nabla_{w_t} \\hat{q}(s_t, a_t, w_t)\n",
    "\\end{equation}\n",
    "where \\{$s_t$, $a_t$, $r_t$, $s_{t+1}$\\} is uniformly sampled from the experience replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Agent():\n",
    "    \n",
    "    \"\"\" Since the discrete actions have been redefined as {0,1} by env, we can simply represent the action by a number. \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 Q_func: torch.nn.Module, \n",
    "                 action_dim: int,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 replay_buffer: collections.deque, # Object of ReplayBuffer\n",
    "                 replay_start_size: int, # The number of experiences stored in the replay buffer before learning starts\n",
    "                 batch_size: int, # The number of experiences to sample from the replay buffer for every learning iteration\n",
    "                 replay_frequent :int, # Train the network every {replay_frequent} steps, which would also help to decorrelate the samples\n",
    "                 epsilon:float = 0.1,\n",
    "                 gamma:float = 0.9,\n",
    "                 device:torch.device = torch.device(\"cpu\")\n",
    "                 ) -> None:\n",
    "        self.device = device\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.replay_start_size = replay_start_size\n",
    "        self.batch_size = batch_size\n",
    "        self.replay_frequent = replay_frequent\n",
    "        \n",
    "        \"\"\"Here, we set a Global Counter for **interactions with the environment**, which is used to determine when to update the target network.\"\"\"\n",
    "        self.exp_counter = 0 \n",
    "        \n",
    "        self.Q_func = Q_func\n",
    "        self.criteria = torch.nn.MSELoss()\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def get_target_action(self,obs:np.ndarray) -> int:\n",
    "        obs = torch.tensor(obs,dtype=torch.float32,device=self.device)\n",
    "        Q_list = self.Q_func(obs)\n",
    "        action = torch.argmax(Q_list).item()\n",
    "        return action\n",
    "\n",
    "    def get_behavior_action(self,obs:np.ndarray) -> int:\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = np.random.choice(self.action_dim)\n",
    "        else:\n",
    "            action = self.get_target_action(obs)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def batch_Q_approximation(self,\n",
    "                              batch_obs:np.ndarray,\n",
    "                              batch_action:np.ndarray,\n",
    "                              batch_reward:np.ndarray,\n",
    "                              batch_next_obs:np.ndarray,\n",
    "                              batch_terminated:np.ndarray) -> None:\n",
    "        \n",
    "        \"\"\" Update the Q function by minimizing the TD-error \"\"\"\n",
    "        batch_current_Q = torch.gather(self.Q_func(batch_obs),1,batch_action).squeeze(1)\n",
    "        batch_TD_target = batch_reward + (1-batch_terminated) * self.gamma * self.Q_func(batch_next_obs).max(1)[0] # torch.max returns a tuple (max_value, index_of_max_value)\n",
    "        loss = self.criteria(batch_current_Q,batch_TD_target)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "            \n",
    "    def Q_approximation(self,\n",
    "                        obs:np.ndarray,\n",
    "                        action:int,\n",
    "                        reward:float,\n",
    "                        next_obs:np.ndarray,\n",
    "                        terminated:bool) -> None:\n",
    "        \"\"\"After each interaction with the environment, we call this function and check whether the replay buffer has enough experiences to start learning.\"\"\"\n",
    "        self.exp_counter += 1 # Update the Global Counter here, since we defined the counter in Agent class.\n",
    "        self.replay_buffer.append((obs,action,reward,next_obs,terminated)) # Store the experience in the replay buffer\n",
    "        \n",
    "        # Start learning after the replay buffer has enough experiences\n",
    "        if len(self.replay_buffer) > self.replay_start_size and self.exp_counter%self.replay_frequent == 0: \n",
    "            self.batch_Q_approximation(*self.replay_buffer.sample(self.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Network(torch.nn.Module):\n",
    "    def __init__(self,obs_dim:int,action_dim) -> None:\n",
    "        super(Q_Network,self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(obs_dim,64)\n",
    "        self.fc2 = torch.nn.Linear(64,64)\n",
    "        self.fc3 = torch.nn.Linear(64,action_dim)\n",
    "            \n",
    "    def forward(self,x:torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use python build-in libratory collections to implement the experience replay buffer. The deque is a double-ended queue. It can be used to add or remove elements from both ends. More details can be found in https://docs.python.org/2/library/collections.html#collections.deque. \n",
    "\n",
    "In this code, the replay will start after collecting certain number of experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self,capacity:int,device:torch.device = torch.device(\"cpu\")) -> None:\n",
    "        \"\"\"The parameter \"capacity\" is the maximum number of experiences that can be stored in the replay buffer. \n",
    "            If the number of experiences exceeds the capacity, the oldest experiences will be removed.\"\"\"\n",
    "        self.device = device\n",
    "        \"\"\"Here, we use deque to implement the replay buffer. \n",
    "            Collections.deque is a double-ended queue, which supports adding and removing elements from either end.\"\"\"\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "        \n",
    "    def append(self,exp_data:tuple) -> None:\n",
    "        self.buffer.append(exp_data)\n",
    "        \n",
    "    def sample(self,batch_size:int) -> Tuple[torch.Tensor,torch.Tensor,torch.Tensor,torch.Tensor,torch.Tensor]:\n",
    "        \"\"\"Here, we use random.sample to randomly select a batch of experiences from the replay buffer.\n",
    "            Note that the return type of random.sample is a list, we need to convert it to a numpy array to avoid low efficiency\"\"\"\n",
    "        mini_batch = random.sample(self.buffer,batch_size)\n",
    "        obs_batch, action_batch, reward_batch, next_obs_batch, terminated_batch = zip(*mini_batch)\n",
    "        \n",
    "        obs_batch = np.array(obs_batch)\n",
    "        next_obs_batch = np.array(next_obs_batch)\n",
    "        \n",
    "        obs_batch = torch.tensor(obs_batch,dtype=torch.float32,device=self.device)\n",
    "        \n",
    "        action_batch = torch.tensor(action_batch,dtype=torch.int64,device=self.device) # for gather function, the index should be int type\n",
    "        action_batch = action_batch.unsqueeze(1)\n",
    "        \n",
    "        reward_batch = torch.tensor(reward_batch,dtype=torch.float32,device=self.device)\n",
    "        next_obs_batch = torch.tensor(next_obs_batch,dtype=torch.float32,device=self.device)\n",
    "        terminated_batch = torch.tensor(terminated_batch,dtype=torch.int64,device=self.device)\n",
    "          \n",
    "        return obs_batch, action_batch, reward_batch, next_obs_batch, terminated_batch\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainManager():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 env:gym.Env,\n",
    "                 episode_num:int = 1000,\n",
    "                 lr:float = 0.001,\n",
    "                 gamma:float = 0.9,\n",
    "                 epsilon:float = 0.1,\n",
    "                 buffer_capacity:int = 2000,\n",
    "                 replay_start_size:int = 200,\n",
    "                 replay_frequent:int = 4,\n",
    "                 batch_size:int = 32) -> None:\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # self.device = torch.device(\"cpu\")\n",
    "        self.env = env\n",
    "        self.episode_num = episode_num\n",
    "        obs_dim = gym.spaces.utils.flatdim(env.observation_space) \n",
    "        action_dim = env.action_space.n\n",
    "        self.buffer = ReplayBuffer(capacity=buffer_capacity,device=self.device)\n",
    "        Q_func = Q_Network(obs_dim,action_dim).to(self.device)\n",
    "        optimizer = torch.optim.Adam(Q_func.parameters(),lr=lr)\n",
    "        self.agent = DQN_Agent(Q_func = Q_func,\n",
    "                               action_dim = action_dim,\n",
    "                               optimizer = optimizer,\n",
    "                               replay_buffer = self.buffer,\n",
    "                               replay_start_size = replay_start_size,\n",
    "                               batch_size = batch_size,\n",
    "                               replay_frequent=replay_frequent,\n",
    "                               epsilon = epsilon,\n",
    "                               gamma = gamma,\n",
    "                               device = self.device)\n",
    "        \n",
    "    def train_episode(self,is_render:bool=False) -> float:\n",
    "        total_reward = 0 \n",
    "        obs,_ = self.env.reset() \n",
    "        while True:\n",
    "            action = self.agent.get_behavior_action(obs) \n",
    "            next_obs, reward, terminated, _, _ = self.env.step(action) \n",
    "            total_reward += reward \n",
    "            self.agent.Q_approximation(obs,action,reward,next_obs,terminated)\n",
    "            obs = next_obs\n",
    "            if is_render:\n",
    "                self.env.render()\n",
    "                                \n",
    "            if terminated:\n",
    "                break\n",
    "            \n",
    "        return total_reward       \n",
    "\n",
    "    def test_episode(self) -> float:\n",
    "        total_reward = 0 \n",
    "        obs,_ = self.env.reset()\n",
    "        while True:\n",
    "            action = self.agent.get_target_action(obs) \n",
    "            next_obs, reward, terminated, _, _= self.env.step(action) \n",
    "            total_reward += reward\n",
    "            self.env.render()\n",
    "            if terminated: break\n",
    "            \n",
    "        return total_reward\n",
    "            \n",
    "            \n",
    "    def train(self) -> None:        \n",
    "        for e in range(self.episode_num):\n",
    "            episode_reward = self.train_episode()\n",
    "            print('Episode %s: Total Reward = %.2f'%(e,episode_reward)) \n",
    "            \n",
    "            if e % 100 == 0: \n",
    "                test_reward = self.test_episode()\n",
    "                print('Test Total Reward = %.2f'%(test_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v1')\n",
    "    Manger = TrainManager(env = env,\n",
    "                        episode_num = 500,\n",
    "                        lr = 0.001,\n",
    "                        gamma = 0.9,\n",
    "                        epsilon = 0.1\n",
    "                        )\n",
    "    Manger.train()\n",
    "    Manger.plotting()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
